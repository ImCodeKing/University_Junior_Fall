{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 《2023类脑计算和类脑计算系统技术》ANN编程实验（二）\n",
    "### by 林逸晗 邢毅诚 2023年10月14日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行前准备\n",
    "\n",
    "- 安装pytorch\n",
    "- 安装tensorflow, keras\n",
    "    - conda install tensorflow\n",
    "    - conda install keras\n",
    "    版本不限，我们只是需要它的数据接口\n",
    "- 安装matplotlib\n",
    "    - pip install matplotlib\n",
    "    用来画图，版本不限\n",
    "    \n",
    "    \n",
    "- 下载[imdb.npz](https://www.kaggle.com/datasets/vikramtiwari/imdb-dataset-for-keras-imdbnpz)，放置在notebook同路径下（如果不在服务器上运行，可以跳过此步骤，会自动下载）\n",
    "- 直接在jupyter内运行下面的bash命令\n",
    "- 如果在服务器上运行，请把以下的所有\"cuda:0\"，都改成\"cuda:x\"，x为给你分配的组号\n",
    "\n",
    "本代码修改自[github仓库:Pytorch-imdb-classification](https://github.com/Cong-Huang/Pytorch-imdb-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/student/.keras’: File exists\n",
      "mkdir: cannot create directory ‘model’: File exists\n"
     ]
    }
   ],
   "source": [
    "#此部分代码用来下载数据集，如果不在服务器上跑，数据集可以自动下载，并不需要进行这几个操作，如果在后续操作中无法找到数据集的话，请手动在工程目录下创建model文件夹，并将下载后的imdb.npz放入/.keras/datasets文件夹。\n",
    "!mkdir ~/.keras\n",
    "!cp ./imdb.npz ~/.keras/datasets\n",
    "!mkdir model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import * \n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.datasets import imdb \n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力机制与多头注意力机制\n",
    "\n",
    "### [原始论文:Attention Is All You Need,2017](https://arxiv.org/abs/1706.03762v5)\n",
    "\n",
    "\n",
    "![attention.png](./figure/attention.png)\n",
    "左图是单头注意力，右图是多头注意力\n",
    "\n",
    "\n",
    "### 1. 为什么要有Attention:RNN和LSTM有什么问题？\n",
    "\n",
    "Q: 依赖关系不一定是时序关系！全局依赖关系如何获得？\n",
    "\n",
    "让我们从循环神经网络的老大难问题——机器翻译问题入手。我们知道，普通的用目标语言中的词语来代替原文中的对应词语是行不通的，因为从语言到另一种语言时词语的语序会发生变化。比如英语的“red”对应法语的“rouge”，英语的“dress”对应法语“robe”，但是英语的“red dress”对应法语的“robe rouge”。\n",
    "\n",
    "为了解决这个问题，我们创造了Encoder-Decoder结构的循环神经网络。它先通过一个Encoder循环神经网络读入所有的待翻译句子中的单词，得到一个包含原文所有信息的中间隐藏层，接着把中间隐藏层状态输入Decoder网络，一个词一个词的输出翻译句子。这样子无论输入中的关键词语有着怎样的先后次序，由于都被打包到中间层一起输入后方网络，我们的Encoder-Decoder网络都可以很好地处理这些词的输出位置和形式了。\n",
    "\n",
    "##### Q: 为什么注意力机制看似简单却这么晚才提出？\n",
    "\n",
    "算力的发展使得这种稠密计算在GPU性能大幅跃升之后才能得以应用到DNN中\n",
    "\n",
    "### 2. 注意力机制的公式化和理解\n",
    "\n",
    "\n",
    "可以注意到输入是query、key、value三个矩阵，首先我们计算query与每个key的关联性（compatibility），之后将每个关联性作为每个value的权重（weight），各个权重与value的乘积相加得到输出，这就是注意力机制的基本方法论。\n",
    "\n",
    "\n",
    "```\n",
    "例：这里我们假定输入的Q、K、V三个张量的数据格式为：[batch_size, s_len, dim]，其中s_len表示输入的句子的长度，则每个单词都可以理解为dim维的向量，注意力就是希望每个单词的向量中都可以包含除了自身之外和其它单词（即上下文之间）的信息，所以我们先将Q与K做矩阵乘法QK^T ，这样我们就得到了一个[batch_size, s_len, s_len] 格式的输出，并对它做一次softmax，这样就得到了一个每个单词之间的关系的得分张量[batch_size, s_len, s_len] ，此时我们得到了各个单词与其它单词之间的关系分数，如果关系越密切，那么数值就应该越大，我们将这个矩阵与value矩阵相乘，就得到了最终的结果。\n",
    "```\n",
    "\n",
    "### 3. Attention 带来的算法改进\n",
    "\n",
    "Attention显著地提高了翻译算法的表现。它可以很好地使Decoder网络注意原文中的某些重要区域来得到更好的翻译。\n",
    "\n",
    "Attention解决了信息瓶颈问题。原先的Encoder-Decoder网络的中间状态只能存储有限的文本信息，现在它只需要完成如何分配注意力的任务即可。\n",
    "\n",
    "Attention减轻了梯度消失问题。Attention在网络后方到前方建立了连接的捷径，使得梯度可以更好的传递。\n",
    "\n",
    "Attention提供了一些可解释性。通过观察网络运行过程中产生的注意力的分布，我们可以知道网络在输出某句话时都把注意力集中在哪里；而且通过训练网络，我们还得到了一个免费的翻译词典（soft alignment）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单头注意力机制的实现（不加权）\n",
    "\n",
    "\n",
    "- 请同学们按照下列公式，补充attention函数代码，并在已有数据上验证代码结果\n",
    "\n",
    "$$Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d}})*V$$\n",
    "\n",
    "其中 Q，K，V是query、key、value，是[数据批次大小, 序列长度, 特征维度]的张量，d是特征维度\n",
    "\n",
    "- **思考1**：QKV中，QKV一定要不同吗？\n",
    "- **思考2**：为什么需要除以特征维度的平方根？\n",
    "```\n",
    "原文：We suspect that for large values of d_k , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by d_k .\n",
    "```\n",
    "这一个解释是合理的吗？不去掉可以吗？\n",
    "- **思考3**：softmax一定是最有道理的吗？可以用其他非线性函数吗？\n",
    "\n",
    "- 可参考：\n",
    "    - [知乎1](https://zhuanlan.zhihu.com/p/380892265)\n",
    "    - [知乎2](https://zhuanlan.zhihu.com/p/611684065?utm_id=0)\n",
    "    \n",
    "结果应该如：\n",
    "![output.png](./figure/output.png)\n",
    "\n",
    "**在此处，请根据attention的定义完成attention函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_attn size: torch.Size([1, 10, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### TODO：这里会作为第一个填空部分 ########\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"Compute 'Scaled Dot Product Attention'\"\"\"\n",
    "    \"\"\"Please write your code here\"\"\"\n",
    "\n",
    "\n",
    "query  = torch.zeros([1,10,32])\n",
    "key    = torch.zeros([1,10,32])\n",
    "value  = torch.zeros([1,10,32])\n",
    "\n",
    "for j in range(10):\n",
    "    for i in range(32):\n",
    "        query[0,j,i] = math.sin(i*j)\n",
    "        key[0,j,i] = math.cos(i*j)\n",
    "        value[0,j,i] = math.tan(i*j)\n",
    "\n",
    "weighted_V,p_attn = attention(query, key, value, mask=None, dropout=None)\n",
    "\n",
    "print('p_attn size:',p_attn.shape)\n",
    "\n",
    "plt.imshow(p_attn[0,...],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加权的注意力机制\n",
    "\n",
    "- 实用中发现，纯靠这一个公式效果并不一定很好，太依赖于先验设计，因此可以把Q、k、V都映射到隐空间中，加权后进行注意力\n",
    "\n",
    "\n",
    "$$Attention(Q,K,V) = softmax(\\frac{QW_Q * (KW_K)^T}{\\sqrt{d}})*VW_V $$\n",
    "\n",
    "在这里，我们用一个pytorch的module来实现这样一个带权重，可学习的Attention module\n",
    "\n",
    "**在此处，请根单头注意力的定义完成single_head_attention()函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_attn size: torch.Size([1, 10, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### TODO：这里会作为第2个填空部分 ########\n",
    "class single_head_attention(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_dim, hidden_dim, dropout):\n",
    "        super(single_head_attention, self).__init__()\n",
    "        self.wq = nn.Parameter(torch.randn(feature_dim,hidden_dim))\n",
    "        self.wk = nn.Parameter(torch.randn(feature_dim,hidden_dim))\n",
    "        self.wv = nn.Parameter(torch.randn(feature_dim,hidden_dim))\n",
    "        \n",
    "        #试试去掉以下三行初始化，看看会发生什么\n",
    "        nn.init.xavier_uniform_(self.wq)\n",
    "        nn.init.xavier_uniform_(self.wk)\n",
    "        nn.init.xavier_uniform_(self.wv)\n",
    "\n",
    "    def forward(self,query, key, value, mask=None, dropout=None):\n",
    "        \"\"\"Compute 'Scaled Dot Product Attention'\"\"\"\n",
    "        \"\"\"Please write your code here\"\"\"\n",
    "        \n",
    "sha = single_head_attention(32,16, dropout=None)\n",
    "\n",
    "query  = torch.zeros([1,10,32])\n",
    "key    = torch.zeros([1,10,32])\n",
    "value  = torch.zeros([1,10,32])\n",
    "\n",
    "for j in range(10):\n",
    "    for i in range(32):\n",
    "        query[0,j,i] = math.sin(i*j)\n",
    "        key[0,j,i] = math.cos(i*j)\n",
    "        value[0,j,i] = math.tan(i*j)\n",
    "\n",
    "weighted_V,p_attn = sha(query, key, value, mask=None)\n",
    "\n",
    "print('p_attn size:',p_attn.shape)\n",
    "\n",
    "plt.imshow(p_attn[0,...].detach(),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多头注意力机制的实现\n",
    "\n",
    "- 单头注意力一次性要把所有的特征都进行点积，计算量上是不是有点太夸张了?\n",
    "- 于是发展出了多头注意力机制，可作为参考\n",
    "- [MTA deep_learning_implementations](https://github.com/laohuu/deep_learning_implementations/blob/main/algorithms/transformers/mha.py#)\n",
    "\n",
    "# 实验：训练一个LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力机制在LSTM上的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baselineModel:LSTM\n",
    "# 这个模型的准确率是87%左右\n",
    "class LSTM_basic(nn.Module):\n",
    "    def __init__(self, max_words, emb_size, hid_size, dropout):\n",
    "        super(LSTM_basic, self).__init__()\n",
    "        self.max_words = max_words\n",
    "        self.emb_size = emb_size\n",
    "        self.hid_size = hid_size\n",
    "        self.dropout = dropout\n",
    "        self.Embedding = nn.Embedding(self.max_words, self.emb_size)\n",
    "        self.LSTM = nn.LSTM(self.emb_size, self.hid_size, num_layers=2,\n",
    "                            batch_first=True, bidirectional=True)   # 2层双向LSTM\n",
    "        self.dp = nn.Dropout(self.dropout)\n",
    "        self.fc1 = nn.Linear(self.hid_size*2, self.hid_size)\n",
    "        self.fc2 = nn.Linear(self.hid_size, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input : [bs, maxlen]\n",
    "        output: [bs, 2] \n",
    "        \"\"\"\n",
    "        x = self.Embedding(x)  # [bs, ml, emb_size]\n",
    "        x = self.dp(x)\n",
    "        x, _ = self.LSTM(x)  # [bs, ml, 2*hid_size]\n",
    "        x = self.dp(x)\n",
    "        x = F.relu(self.fc1(x))   # [bs, ml, hid_size]\n",
    "        x = F.avg_pool2d(x, (x.shape[1], 1)).squeeze()  # [bs, 1, hid_size] => [bs, hid_size]\n",
    "        out = self.fc2(x)    # [bs, 2]\n",
    "        return out  # [bs, 2]\n",
    "\n",
    "class LSTM_Attention(nn.Module):\n",
    "    def __init__(self, max_words, emb_size, hid_size, dropout):\n",
    "        super(LSTM_Attention, self).__init__()\n",
    "        self.max_words = max_words\n",
    "        self.emb_size = emb_size\n",
    "        self.hid_size = hid_size\n",
    "        self.dropout = dropout\n",
    "        self.Embedding = nn.Embedding(self.max_words, self.emb_size)\n",
    "        self.LSTM1 = nn.LSTM(self.emb_size, self.hid_size, num_layers=1,\n",
    "                            batch_first=True, bidirectional=True)   # 2层双向LSTM\n",
    "        self.attn = single_head_attention(2*hid_size,hid_size, dropout=None)\n",
    "        self.LSTM2 = nn.LSTM(self.emb_size, self.hid_size, num_layers=1,\n",
    "                            batch_first=True, bidirectional=True)   # 2层双向LSTM\n",
    "        \n",
    "        self.dp = nn.Dropout(self.dropout)\n",
    "        self.fc1 = nn.Linear(self.hid_size*2, self.hid_size)\n",
    "        self.fc2 = nn.Linear(self.hid_size, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input : [bs, maxlen]\n",
    "        output: [bs, 2] \n",
    "        \"\"\"\n",
    "        x = self.Embedding(x)  # [bs, ml, emb_size]\n",
    "        x = self.dp(x)\n",
    "        x, _ = self.LSTM1(x)  # [bs, ml, 2*hid_size]\n",
    "        x,_ = self.attn(x,x,x)\n",
    "        x, _ = self.LSTM2(x)  # [bs, ml, 2*hid_size]\n",
    "        \n",
    "        x = self.dp(x)\n",
    "        x = F.relu(self.fc1(x))   # [bs, ml, hid_size]\n",
    "        x = F.avg_pool2d(x, (x.shape[1], 1)).squeeze()  # [bs, 1, hid_size] => [bs, hid_size]\n",
    "        out = self.fc2(x)    # [bs, 2]\n",
    "        return out  # [bs, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据和训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 256) (25000, 256)\n"
     ]
    }
   ],
   "source": [
    "MAX_WORDS = 10000  # imdb’s vocab_size 即词汇表大小\n",
    "MAX_LEN = 256      # max length\n",
    "BATCH_SIZE = 256\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\"./imdb.npz\",num_words=MAX_WORDS)\n",
    "x_train = pad_sequences(x_train, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "print(x_train.shape, x_test.shape)\n",
    "\n",
    "# 转化为TensorDataset\n",
    "train_data = TensorDataset(torch.LongTensor(x_train), torch.LongTensor(y_train))\n",
    "test_data = TensorDataset(torch.LongTensor(x_test), torch.LongTensor(y_test))\n",
    "\n",
    "# 转化为 DataLoader\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):   # 训练模型\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        y_ = model(x)\n",
    "        loss = criterion(y_, y)  # 得到loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx + 1) % 10 == 0:    # 打印loss\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            \n",
    "def test(model, device, test_loader):    # 测试模型\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')  # 累加loss\n",
    "    test_loss = 0.0 \n",
    "    acc = 0 \n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            y_ = model(x)\n",
    "        test_loss += criterion(y_, y)\n",
    "        pred = y_.max(-1, keepdim=True)[1]   # .max() 2输出，分别为最大值和最大值的index\n",
    "        acc += pred.eq(y.view_as(pred)).sum().item()    # 记得加item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, acc, len(test_loader.dataset),\n",
    "        100. * acc / len(test_loader.dataset)))\n",
    "    return acc / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Attention(\n",
      "  (Embedding): Embedding(10000, 128)\n",
      "  (LSTM1): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
      "  (attn): single_head_attention()\n",
      "  (LSTM2): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
      "  (dp): Dropout(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [2304/25000 (9%)]\tLoss: 0.703893\n",
      "Train Epoch: 1 [4864/25000 (19%)]\tLoss: 0.694157\n",
      "Train Epoch: 1 [7424/25000 (30%)]\tLoss: 0.683462\n",
      "Train Epoch: 1 [9984/25000 (40%)]\tLoss: 0.651767\n",
      "Train Epoch: 1 [12544/25000 (50%)]\tLoss: 0.614012\n",
      "Train Epoch: 1 [15104/25000 (60%)]\tLoss: 0.527798\n",
      "Train Epoch: 1 [17664/25000 (70%)]\tLoss: 0.550648\n",
      "Train Epoch: 1 [20224/25000 (81%)]\tLoss: 0.621355\n",
      "Train Epoch: 1 [22784/25000 (91%)]\tLoss: 0.455265\n",
      "\n",
      "Test set: Average loss: 0.4813, Accuracy: 19316/25000 (77%)\n",
      "acc is: 0.7726, best acc is 0.7726\n",
      "\n",
      "Train Epoch: 2 [2304/25000 (9%)]\tLoss: 0.421954\n",
      "Train Epoch: 2 [4864/25000 (19%)]\tLoss: 0.497478\n",
      "Train Epoch: 2 [7424/25000 (30%)]\tLoss: 0.462233\n",
      "Train Epoch: 2 [9984/25000 (40%)]\tLoss: 0.379028\n",
      "Train Epoch: 2 [12544/25000 (50%)]\tLoss: 0.400045\n",
      "Train Epoch: 2 [15104/25000 (60%)]\tLoss: 0.395063\n",
      "Train Epoch: 2 [17664/25000 (70%)]\tLoss: 0.398652\n",
      "Train Epoch: 2 [20224/25000 (81%)]\tLoss: 0.400658\n",
      "Train Epoch: 2 [22784/25000 (91%)]\tLoss: 0.385302\n",
      "\n",
      "Test set: Average loss: 0.3749, Accuracy: 20868/25000 (83%)\n",
      "acc is: 0.8347, best acc is 0.8347\n",
      "\n",
      "Train Epoch: 3 [2304/25000 (9%)]\tLoss: 0.330275\n",
      "Train Epoch: 3 [4864/25000 (19%)]\tLoss: 0.368838\n",
      "Train Epoch: 3 [7424/25000 (30%)]\tLoss: 0.342478\n",
      "Train Epoch: 3 [9984/25000 (40%)]\tLoss: 0.303857\n",
      "Train Epoch: 3 [12544/25000 (50%)]\tLoss: 0.461541\n",
      "Train Epoch: 3 [15104/25000 (60%)]\tLoss: 0.312457\n",
      "Train Epoch: 3 [17664/25000 (70%)]\tLoss: 0.345230\n",
      "Train Epoch: 3 [20224/25000 (81%)]\tLoss: 0.329515\n",
      "Train Epoch: 3 [22784/25000 (91%)]\tLoss: 0.363510\n",
      "\n",
      "Test set: Average loss: 0.3302, Accuracy: 21435/25000 (86%)\n",
      "acc is: 0.8574, best acc is 0.8574\n",
      "\n",
      "Train Epoch: 4 [2304/25000 (9%)]\tLoss: 0.344867\n",
      "Train Epoch: 4 [4864/25000 (19%)]\tLoss: 0.262186\n",
      "Train Epoch: 4 [7424/25000 (30%)]\tLoss: 0.331163\n",
      "Train Epoch: 4 [9984/25000 (40%)]\tLoss: 0.294531\n",
      "Train Epoch: 4 [12544/25000 (50%)]\tLoss: 0.287803\n",
      "Train Epoch: 4 [15104/25000 (60%)]\tLoss: 0.249361\n",
      "Train Epoch: 4 [17664/25000 (70%)]\tLoss: 0.282080\n",
      "Train Epoch: 4 [20224/25000 (81%)]\tLoss: 0.303593\n",
      "Train Epoch: 4 [22784/25000 (91%)]\tLoss: 0.292585\n",
      "\n",
      "Test set: Average loss: 0.3199, Accuracy: 21591/25000 (86%)\n",
      "acc is: 0.8636, best acc is 0.8636\n",
      "\n",
      "Train Epoch: 5 [2304/25000 (9%)]\tLoss: 0.267855\n",
      "Train Epoch: 5 [4864/25000 (19%)]\tLoss: 0.261038\n",
      "Train Epoch: 5 [7424/25000 (30%)]\tLoss: 0.288121\n",
      "Train Epoch: 5 [9984/25000 (40%)]\tLoss: 0.248889\n",
      "Train Epoch: 5 [12544/25000 (50%)]\tLoss: 0.310874\n",
      "Train Epoch: 5 [15104/25000 (60%)]\tLoss: 0.291335\n",
      "Train Epoch: 5 [17664/25000 (70%)]\tLoss: 0.226636\n",
      "Train Epoch: 5 [20224/25000 (81%)]\tLoss: 0.253903\n",
      "Train Epoch: 5 [22784/25000 (91%)]\tLoss: 0.237063\n",
      "\n",
      "Test set: Average loss: 0.3147, Accuracy: 21666/25000 (87%)\n",
      "acc is: 0.8666, best acc is 0.8666\n",
      "\n",
      "Train Epoch: 6 [2304/25000 (9%)]\tLoss: 0.292643\n",
      "Train Epoch: 6 [4864/25000 (19%)]\tLoss: 0.299111\n",
      "Train Epoch: 6 [7424/25000 (30%)]\tLoss: 0.348348\n",
      "Train Epoch: 6 [9984/25000 (40%)]\tLoss: 0.264494\n",
      "Train Epoch: 6 [12544/25000 (50%)]\tLoss: 0.232229\n",
      "Train Epoch: 6 [15104/25000 (60%)]\tLoss: 0.193689\n",
      "Train Epoch: 6 [17664/25000 (70%)]\tLoss: 0.220386\n",
      "Train Epoch: 6 [20224/25000 (81%)]\tLoss: 0.266114\n",
      "Train Epoch: 6 [22784/25000 (91%)]\tLoss: 0.345478\n",
      "\n",
      "Test set: Average loss: 0.3147, Accuracy: 21665/25000 (87%)\n",
      "acc is: 0.8666, best acc is 0.8666\n",
      "\n",
      "Train Epoch: 7 [2304/25000 (9%)]\tLoss: 0.203969\n",
      "Train Epoch: 7 [4864/25000 (19%)]\tLoss: 0.256935\n",
      "Train Epoch: 7 [7424/25000 (30%)]\tLoss: 0.274644\n",
      "Train Epoch: 7 [9984/25000 (40%)]\tLoss: 0.370448\n",
      "Train Epoch: 7 [12544/25000 (50%)]\tLoss: 0.290169\n",
      "Train Epoch: 7 [15104/25000 (60%)]\tLoss: 0.232217\n",
      "Train Epoch: 7 [17664/25000 (70%)]\tLoss: 0.260132\n",
      "Train Epoch: 7 [20224/25000 (81%)]\tLoss: 0.220158\n",
      "Train Epoch: 7 [22784/25000 (91%)]\tLoss: 0.301801\n",
      "\n",
      "Test set: Average loss: 0.3106, Accuracy: 21752/25000 (87%)\n",
      "acc is: 0.8701, best acc is 0.8701\n",
      "\n",
      "Train Epoch: 8 [2304/25000 (9%)]\tLoss: 0.251161\n",
      "Train Epoch: 8 [4864/25000 (19%)]\tLoss: 0.260259\n",
      "Train Epoch: 8 [7424/25000 (30%)]\tLoss: 0.247375\n",
      "Train Epoch: 8 [9984/25000 (40%)]\tLoss: 0.309387\n",
      "Train Epoch: 8 [12544/25000 (50%)]\tLoss: 0.258725\n",
      "Train Epoch: 8 [15104/25000 (60%)]\tLoss: 0.260614\n",
      "Train Epoch: 8 [17664/25000 (70%)]\tLoss: 0.279732\n",
      "Train Epoch: 8 [20224/25000 (81%)]\tLoss: 0.265276\n",
      "Train Epoch: 8 [22784/25000 (91%)]\tLoss: 0.251308\n",
      "\n",
      "Test set: Average loss: 0.3135, Accuracy: 21699/25000 (87%)\n",
      "acc is: 0.8680, best acc is 0.8701\n",
      "\n",
      "Train Epoch: 9 [2304/25000 (9%)]\tLoss: 0.286027\n",
      "Train Epoch: 9 [4864/25000 (19%)]\tLoss: 0.235239\n",
      "Train Epoch: 9 [7424/25000 (30%)]\tLoss: 0.282165\n",
      "Train Epoch: 9 [9984/25000 (40%)]\tLoss: 0.288903\n",
      "Train Epoch: 9 [12544/25000 (50%)]\tLoss: 0.251977\n",
      "Train Epoch: 9 [15104/25000 (60%)]\tLoss: 0.310342\n",
      "Train Epoch: 9 [17664/25000 (70%)]\tLoss: 0.239924\n",
      "Train Epoch: 9 [20224/25000 (81%)]\tLoss: 0.239219\n",
      "Train Epoch: 9 [22784/25000 (91%)]\tLoss: 0.342502\n",
      "\n",
      "Test set: Average loss: 0.2961, Accuracy: 21902/25000 (88%)\n",
      "acc is: 0.8761, best acc is 0.8761\n",
      "\n",
      "Train Epoch: 10 [2304/25000 (9%)]\tLoss: 0.231299\n",
      "Train Epoch: 10 [4864/25000 (19%)]\tLoss: 0.301041\n",
      "Train Epoch: 10 [7424/25000 (30%)]\tLoss: 0.231603\n",
      "Train Epoch: 10 [9984/25000 (40%)]\tLoss: 0.268930\n",
      "Train Epoch: 10 [12544/25000 (50%)]\tLoss: 0.264555\n",
      "Train Epoch: 10 [15104/25000 (60%)]\tLoss: 0.267102\n",
      "Train Epoch: 10 [17664/25000 (70%)]\tLoss: 0.284845\n",
      "Train Epoch: 10 [20224/25000 (81%)]\tLoss: 0.272158\n",
      "Train Epoch: 10 [22784/25000 (91%)]\tLoss: 0.254028\n",
      "\n",
      "Test set: Average loss: 0.3745, Accuracy: 21260/25000 (85%)\n",
      "acc is: 0.8504, best acc is 0.8761\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "EMB_SIZE = 128   # embedding size\n",
    "HID_SIZE = 128   # lstm hidden size\n",
    "DROPOUT = 0.2 \n",
    "EPOCH = 10\n",
    "INITIAL_LEARNING_RATE = 1e-3\n",
    "\n",
    "# model = LSTM_basic(MAX_WORDS, EMB_SIZE, HID_SIZE, DROPOUT).to(DEVICE)\n",
    "model = LSTM_Attention(MAX_WORDS, EMB_SIZE, HID_SIZE, DROPOUT).to(DEVICE)\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=INITIAL_LEARNING_RATE, betas=(0.9, 0.995), eps=1e-08, weight_decay=1e-5)\n",
    "best_acc = 0.0 \n",
    "PATH = 'model/model.pth'  # 定义模型保存路径\n",
    "\n",
    "acc_list = []\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCH//2, eta_min=1e-8)\n",
    "for epoch in range(1, EPOCH+1):  # 10个epoch\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    acc = test(model, DEVICE, test_loader)\n",
    "    acc_list.append(acc)\n",
    "    if best_acc < acc: \n",
    "        best_acc = acc \n",
    "        torch.save(model.state_dict(), PATH)\n",
    "    print(\"acc is: {:.4f}, best acc is {:.4f}\\n\".format(acc, best_acc)) \n",
    "    scheduler.step()\n",
    "    \n",
    "plt.plot(acc_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2961, Accuracy: 21902/25000 (88%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87608"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检验保存的模型\n",
    "best_model = LSTM_Attention(MAX_WORDS, EMB_SIZE, HID_SIZE, DROPOUT).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(PATH))\n",
    "test(best_model, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现一个简单的Transformer\n",
    "- trasnformer 对小数据集未必友好*\n",
    "- 参考资料：\n",
    "\n",
    "    [https://github.com/hyunwoongko/transformer/tree/master](https://github.com/hyunwoongko/transformer/tree/master)\n",
    "    \n",
    "    [https://e2eml.school/transformers.html#softmax](https://e2eml.school/transformers.html#softmax)\n",
    "    \n",
    "Transformer是在\"Attention is all you need\"神经网络中提出的神经网络模型，其结构如下图所示：\n",
    "![transformer.png](./figure/transformer.png)\n",
    "\n",
    "Transformer模型，分为左侧的Encoder和右侧的Decoder两部分，在本次实验中，由于我们只进行一个二分类的情感分析任务，因此并不需要编写太多的Decorder内容。\n",
    "\n",
    "本实验所实现的神经网络共有以下几个部分：\n",
    "\n",
    "- Word Embedding\n",
    "- 位置编码\n",
    "- Encoder实现\n",
    "    - Feed Forward层\n",
    "    - Add Norm层\n",
    "    - Encoder block实现\n",
    "    \n",
    "    \n",
    "## Word Embedding\n",
    "Word Embedding会将一个单词转变为可以进行计算的向量，其将一个单词转映射成了低维的连续向量。如\n",
    "```\n",
    "cat: (-0.065, -0.035, 0.019, -0.026, 0.085,…)\n",
    "dog: (-0.019, -0.076, 0.044, 0.021,0.095,…)\n",
    "table: (0.027, 0.013, 0.006, -0.023, 0.014, …)\n",
    "```\n",
    "Word Embedding的作用是单词转变成了可以计算的向量。如：\n",
    "\n",
    "V(\"King\") - V(\"Male\") +  V(\"Female\") 约等于 V(\"Queen\")\n",
    "\n",
    "使用这种方式，可以使得一个单词可以作为神经网络的输入被计算。\n",
    "\n",
    "本实验已经实现好了Word Embedding的实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 位置编码\n",
    "\n",
    "我们使用位置编码来表示单词出现在句子中的位置，由于Transformer不采用RNN的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于NLP来说非常重要。所以Transformer中使用位置Embedding保存单词在序列中的相对或绝对位置。\n",
    "\n",
    "位置编码用PE表示，PE的维度与单词Embedding是一样的。PE可以通过训练得到，也可以使用某种公式计算得到。我们在Transformer中采用了后者，其计算公式如下\n",
    "\n",
    "$PE_{pos,2i} = sin(pos/10000^{2i/d})$\n",
    "\n",
    "$PE_{pos,2i+1} = cos(pos/10000^{2i/d})$\n",
    "\n",
    "其中Pos表示单词在句子中的位置，d表示PE的维度（与词Embedding一样），在此处，需要你根据公式完成对位置编码函数的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \"\"\"Please write your code here\"\"\"\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eecoder实现\n",
    "Encoder block如图所示，\n",
    "![Encoder](./figure/encoder.png)\n",
    "由若干个block组成，可以看到其实由Multi-Head Attention, Add & Norm, Feed Forward, Add & Norm组成的。（在本实验中，我们暂且使用单头注意力机制来代替多头注意力机制）\n",
    "\n",
    "## Feed Forward层实现\n",
    "Feed Forward层是一个两层的全连接层，第一层的激活函数为Relu，第二层不适用激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size * 4)\n",
    "        self.linear2 = nn.Linear(hidden_size * 4, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add & Norm 和 Encoder Block实现\n",
    "单个Encoder Block的结构如上图画红框所示, Add & Norm层由Add和Norm两部分组成，其计算公式如下：\n",
    "\n",
    "$LayerNorm(X+SingleHeadAttention(X))$\n",
    "\n",
    "$LayerNorm(X + FeedForward(X))$\n",
    "\n",
    "其中X代表输入，Add是一种残差链接，通常用于解决多层网络训练的问题，而LayerNorm指的是Layer Normalization，其作用是将每一层神经元的输入都转成相同的均值和方差，此模块已经在下面的self.layer_norm定义，不需要自行编写。\n",
    "\n",
    "**在本模块中，需要你根据Encoder的结构编写add & norm以及单个Encoder Block实现。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.attention = single_head_attention(hidden_size,hidden_size,dropout=0.2)\n",
    "        self.feed_forward = FeedForward(hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Please write your code here\"\"\"\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer结构实现\n",
    "在实现了所有的基础模块后，将网络组合起来，即完成了Transformer网络的编写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerTransformer(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, num_layers, output_size):\n",
    "        super(MultiLayerTransformer, self).__init__()\n",
    "        self.Embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_size, dropout=0.1, max_len=10000)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerLayer(hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x = self.Embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            x = transformer_layer(x)\n",
    "        x = F.avg_pool2d(x, (x.shape[1], 1)).squeeze()  # [bs, 1, hid_size] => [bs, hid_size]\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerTransformer(\n",
      "  (Embedding): Embedding(10000, 128)\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_layers): ModuleList(\n",
      "    (0): TransformerLayer(\n",
      "      (attention): single_head_attention()\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): TransformerLayer(\n",
      "      (attention): single_head_attention()\n",
      "      (feed_forward): FeedForward(\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [2304/25000 (9%)]\tLoss: 0.691170\n",
      "Train Epoch: 1 [4864/25000 (19%)]\tLoss: 0.694029\n",
      "Train Epoch: 1 [7424/25000 (30%)]\tLoss: 0.670172\n",
      "Train Epoch: 1 [9984/25000 (40%)]\tLoss: 0.631191\n",
      "Train Epoch: 1 [12544/25000 (50%)]\tLoss: 0.620110\n",
      "Train Epoch: 1 [15104/25000 (60%)]\tLoss: 0.678852\n",
      "Train Epoch: 1 [17664/25000 (70%)]\tLoss: 0.623663\n",
      "Train Epoch: 1 [20224/25000 (81%)]\tLoss: 0.583664\n",
      "Train Epoch: 1 [22784/25000 (91%)]\tLoss: 0.596859\n",
      "\n",
      "Test set: Average loss: 0.5787, Accuracy: 17433/25000 (70%)\n",
      "acc is: 0.6973, best acc is 0.6973\n",
      "\n",
      "Train Epoch: 2 [2304/25000 (9%)]\tLoss: 0.623104\n",
      "Train Epoch: 2 [4864/25000 (19%)]\tLoss: 0.539510\n",
      "Train Epoch: 2 [7424/25000 (30%)]\tLoss: 0.569400\n",
      "Train Epoch: 2 [9984/25000 (40%)]\tLoss: 0.588063\n",
      "Train Epoch: 2 [12544/25000 (50%)]\tLoss: 0.582391\n",
      "Train Epoch: 2 [15104/25000 (60%)]\tLoss: 0.613922\n",
      "Train Epoch: 2 [17664/25000 (70%)]\tLoss: 0.526570\n",
      "Train Epoch: 2 [20224/25000 (81%)]\tLoss: 0.528452\n",
      "Train Epoch: 2 [22784/25000 (91%)]\tLoss: 0.586018\n",
      "\n",
      "Test set: Average loss: 0.5353, Accuracy: 18214/25000 (73%)\n",
      "acc is: 0.7286, best acc is 0.7286\n",
      "\n",
      "Train Epoch: 3 [2304/25000 (9%)]\tLoss: 0.506034\n",
      "Train Epoch: 3 [4864/25000 (19%)]\tLoss: 0.526810\n",
      "Train Epoch: 3 [7424/25000 (30%)]\tLoss: 0.562307\n",
      "Train Epoch: 3 [9984/25000 (40%)]\tLoss: 0.568310\n",
      "Train Epoch: 3 [12544/25000 (50%)]\tLoss: 0.540480\n",
      "Train Epoch: 3 [15104/25000 (60%)]\tLoss: 0.532498\n",
      "Train Epoch: 3 [17664/25000 (70%)]\tLoss: 0.479414\n",
      "Train Epoch: 3 [20224/25000 (81%)]\tLoss: 0.490525\n",
      "Train Epoch: 3 [22784/25000 (91%)]\tLoss: 0.529898\n",
      "\n",
      "Test set: Average loss: 0.5746, Accuracy: 17176/25000 (69%)\n",
      "acc is: 0.6870, best acc is 0.7286\n",
      "\n",
      "Train Epoch: 4 [2304/25000 (9%)]\tLoss: 0.462493\n",
      "Train Epoch: 4 [4864/25000 (19%)]\tLoss: 0.549596\n",
      "Train Epoch: 4 [7424/25000 (30%)]\tLoss: 0.549465\n",
      "Train Epoch: 4 [9984/25000 (40%)]\tLoss: 0.479125\n",
      "Train Epoch: 4 [12544/25000 (50%)]\tLoss: 0.501723\n",
      "Train Epoch: 4 [15104/25000 (60%)]\tLoss: 0.475249\n",
      "Train Epoch: 4 [17664/25000 (70%)]\tLoss: 0.481675\n",
      "Train Epoch: 4 [20224/25000 (81%)]\tLoss: 0.480109\n",
      "Train Epoch: 4 [22784/25000 (91%)]\tLoss: 0.541819\n",
      "\n",
      "Test set: Average loss: 0.5176, Accuracy: 18614/25000 (74%)\n",
      "acc is: 0.7446, best acc is 0.7446\n",
      "\n",
      "Train Epoch: 5 [2304/25000 (9%)]\tLoss: 0.554936\n",
      "Train Epoch: 5 [4864/25000 (19%)]\tLoss: 0.488487\n",
      "Train Epoch: 5 [7424/25000 (30%)]\tLoss: 0.503981\n",
      "Train Epoch: 5 [9984/25000 (40%)]\tLoss: 0.434718\n",
      "Train Epoch: 5 [12544/25000 (50%)]\tLoss: 0.443968\n",
      "Train Epoch: 5 [15104/25000 (60%)]\tLoss: 0.445314\n",
      "Train Epoch: 5 [17664/25000 (70%)]\tLoss: 0.516110\n",
      "Train Epoch: 5 [20224/25000 (81%)]\tLoss: 0.471725\n",
      "Train Epoch: 5 [22784/25000 (91%)]\tLoss: 0.552751\n",
      "\n",
      "Test set: Average loss: 0.4785, Accuracy: 19210/25000 (77%)\n",
      "acc is: 0.7684, best acc is 0.7684\n",
      "\n",
      "Train Epoch: 6 [2304/25000 (9%)]\tLoss: 0.434239\n",
      "Train Epoch: 6 [4864/25000 (19%)]\tLoss: 0.457701\n",
      "Train Epoch: 6 [7424/25000 (30%)]\tLoss: 0.461096\n",
      "Train Epoch: 6 [9984/25000 (40%)]\tLoss: 0.427663\n",
      "Train Epoch: 6 [12544/25000 (50%)]\tLoss: 0.533072\n",
      "Train Epoch: 6 [15104/25000 (60%)]\tLoss: 0.479180\n",
      "Train Epoch: 6 [17664/25000 (70%)]\tLoss: 0.507398\n",
      "Train Epoch: 6 [20224/25000 (81%)]\tLoss: 0.421342\n",
      "Train Epoch: 6 [22784/25000 (91%)]\tLoss: 0.453539\n",
      "\n",
      "Test set: Average loss: 0.4623, Accuracy: 19548/25000 (78%)\n",
      "acc is: 0.7819, best acc is 0.7819\n",
      "\n",
      "Train Epoch: 7 [2304/25000 (9%)]\tLoss: 0.463651\n",
      "Train Epoch: 7 [4864/25000 (19%)]\tLoss: 0.497228\n",
      "Train Epoch: 7 [7424/25000 (30%)]\tLoss: 0.433001\n",
      "Train Epoch: 7 [9984/25000 (40%)]\tLoss: 0.414057\n",
      "Train Epoch: 7 [12544/25000 (50%)]\tLoss: 0.470503\n",
      "Train Epoch: 7 [15104/25000 (60%)]\tLoss: 0.422230\n",
      "Train Epoch: 7 [17664/25000 (70%)]\tLoss: 0.418416\n",
      "Train Epoch: 7 [20224/25000 (81%)]\tLoss: 0.389675\n",
      "Train Epoch: 7 [22784/25000 (91%)]\tLoss: 0.438282\n",
      "\n",
      "Test set: Average loss: 0.4230, Accuracy: 20111/25000 (80%)\n",
      "acc is: 0.8044, best acc is 0.8044\n",
      "\n",
      "Train Epoch: 8 [2304/25000 (9%)]\tLoss: 0.402863\n",
      "Train Epoch: 8 [4864/25000 (19%)]\tLoss: 0.407625\n",
      "Train Epoch: 8 [7424/25000 (30%)]\tLoss: 0.372163\n",
      "Train Epoch: 8 [9984/25000 (40%)]\tLoss: 0.428824\n",
      "Train Epoch: 8 [12544/25000 (50%)]\tLoss: 0.452672\n",
      "Train Epoch: 8 [15104/25000 (60%)]\tLoss: 0.415881\n",
      "Train Epoch: 8 [17664/25000 (70%)]\tLoss: 0.460715\n",
      "Train Epoch: 8 [20224/25000 (81%)]\tLoss: 0.399891\n",
      "Train Epoch: 8 [22784/25000 (91%)]\tLoss: 0.380485\n",
      "\n",
      "Test set: Average loss: 0.4254, Accuracy: 19972/25000 (80%)\n",
      "acc is: 0.7989, best acc is 0.8044\n",
      "\n",
      "Train Epoch: 9 [2304/25000 (9%)]\tLoss: 0.366922\n",
      "Train Epoch: 9 [4864/25000 (19%)]\tLoss: 0.456422\n",
      "Train Epoch: 9 [7424/25000 (30%)]\tLoss: 0.406644\n",
      "Train Epoch: 9 [9984/25000 (40%)]\tLoss: 0.460690\n",
      "Train Epoch: 9 [12544/25000 (50%)]\tLoss: 0.399143\n",
      "Train Epoch: 9 [15104/25000 (60%)]\tLoss: 0.447630\n",
      "Train Epoch: 9 [17664/25000 (70%)]\tLoss: 0.376965\n",
      "Train Epoch: 9 [20224/25000 (81%)]\tLoss: 0.326129\n",
      "Train Epoch: 9 [22784/25000 (91%)]\tLoss: 0.378419\n",
      "\n",
      "Test set: Average loss: 0.4101, Accuracy: 20173/25000 (81%)\n",
      "acc is: 0.8069, best acc is 0.8069\n",
      "\n",
      "Train Epoch: 10 [2304/25000 (9%)]\tLoss: 0.411984\n",
      "Train Epoch: 10 [4864/25000 (19%)]\tLoss: 0.354007\n",
      "Train Epoch: 10 [7424/25000 (30%)]\tLoss: 0.353663\n",
      "Train Epoch: 10 [9984/25000 (40%)]\tLoss: 0.401761\n",
      "Train Epoch: 10 [12544/25000 (50%)]\tLoss: 0.396682\n",
      "Train Epoch: 10 [15104/25000 (60%)]\tLoss: 0.379598\n",
      "Train Epoch: 10 [17664/25000 (70%)]\tLoss: 0.404064\n",
      "Train Epoch: 10 [20224/25000 (81%)]\tLoss: 0.364071\n",
      "Train Epoch: 10 [22784/25000 (91%)]\tLoss: 0.420979\n",
      "\n",
      "Test set: Average loss: 0.4005, Accuracy: 20381/25000 (82%)\n",
      "acc is: 0.8152, best acc is 0.8152\n",
      "\n",
      "Train Epoch: 11 [2304/25000 (9%)]\tLoss: 0.344668\n",
      "Train Epoch: 11 [4864/25000 (19%)]\tLoss: 0.389380\n",
      "Train Epoch: 11 [7424/25000 (30%)]\tLoss: 0.346695\n",
      "Train Epoch: 11 [9984/25000 (40%)]\tLoss: 0.355157\n",
      "Train Epoch: 11 [12544/25000 (50%)]\tLoss: 0.368789\n",
      "Train Epoch: 11 [15104/25000 (60%)]\tLoss: 0.417794\n",
      "Train Epoch: 11 [17664/25000 (70%)]\tLoss: 0.469125\n",
      "Train Epoch: 11 [20224/25000 (81%)]\tLoss: 0.412315\n",
      "Train Epoch: 11 [22784/25000 (91%)]\tLoss: 0.348381\n",
      "\n",
      "Test set: Average loss: 0.3898, Accuracy: 20616/25000 (82%)\n",
      "acc is: 0.8246, best acc is 0.8246\n",
      "\n",
      "Train Epoch: 12 [2304/25000 (9%)]\tLoss: 0.417891\n",
      "Train Epoch: 12 [4864/25000 (19%)]\tLoss: 0.354777\n",
      "Train Epoch: 12 [7424/25000 (30%)]\tLoss: 0.306378\n",
      "Train Epoch: 12 [9984/25000 (40%)]\tLoss: 0.343659\n",
      "Train Epoch: 12 [12544/25000 (50%)]\tLoss: 0.367749\n",
      "Train Epoch: 12 [15104/25000 (60%)]\tLoss: 0.388489\n",
      "Train Epoch: 12 [17664/25000 (70%)]\tLoss: 0.371240\n",
      "Train Epoch: 12 [20224/25000 (81%)]\tLoss: 0.391628\n",
      "Train Epoch: 12 [22784/25000 (91%)]\tLoss: 0.369039\n",
      "\n",
      "Test set: Average loss: 0.3865, Accuracy: 20610/25000 (82%)\n",
      "acc is: 0.8244, best acc is 0.8246\n",
      "\n",
      "Train Epoch: 13 [2304/25000 (9%)]\tLoss: 0.370882\n",
      "Train Epoch: 13 [4864/25000 (19%)]\tLoss: 0.321408\n",
      "Train Epoch: 13 [7424/25000 (30%)]\tLoss: 0.353215\n",
      "Train Epoch: 13 [9984/25000 (40%)]\tLoss: 0.395668\n",
      "Train Epoch: 13 [12544/25000 (50%)]\tLoss: 0.345163\n",
      "Train Epoch: 13 [15104/25000 (60%)]\tLoss: 0.327282\n",
      "Train Epoch: 13 [17664/25000 (70%)]\tLoss: 0.341781\n",
      "Train Epoch: 13 [20224/25000 (81%)]\tLoss: 0.353508\n",
      "Train Epoch: 13 [22784/25000 (91%)]\tLoss: 0.425375\n",
      "\n",
      "Test set: Average loss: 0.3743, Accuracy: 20751/25000 (83%)\n",
      "acc is: 0.8300, best acc is 0.8300\n",
      "\n",
      "Train Epoch: 14 [2304/25000 (9%)]\tLoss: 0.358385\n",
      "Train Epoch: 14 [4864/25000 (19%)]\tLoss: 0.345773\n",
      "Train Epoch: 14 [7424/25000 (30%)]\tLoss: 0.363237\n",
      "Train Epoch: 14 [9984/25000 (40%)]\tLoss: 0.419073\n",
      "Train Epoch: 14 [12544/25000 (50%)]\tLoss: 0.349859\n",
      "Train Epoch: 14 [15104/25000 (60%)]\tLoss: 0.426126\n",
      "Train Epoch: 14 [17664/25000 (70%)]\tLoss: 0.359297\n",
      "Train Epoch: 14 [20224/25000 (81%)]\tLoss: 0.341317\n",
      "Train Epoch: 14 [22784/25000 (91%)]\tLoss: 0.325682\n",
      "\n",
      "Test set: Average loss: 0.3762, Accuracy: 20785/25000 (83%)\n",
      "acc is: 0.8314, best acc is 0.8314\n",
      "\n",
      "Train Epoch: 15 [2304/25000 (9%)]\tLoss: 0.368257\n",
      "Train Epoch: 15 [4864/25000 (19%)]\tLoss: 0.342211\n",
      "Train Epoch: 15 [7424/25000 (30%)]\tLoss: 0.323202\n",
      "Train Epoch: 15 [9984/25000 (40%)]\tLoss: 0.348621\n",
      "Train Epoch: 15 [12544/25000 (50%)]\tLoss: 0.372312\n",
      "Train Epoch: 15 [15104/25000 (60%)]\tLoss: 0.383442\n",
      "Train Epoch: 15 [17664/25000 (70%)]\tLoss: 0.311099\n",
      "Train Epoch: 15 [20224/25000 (81%)]\tLoss: 0.349296\n",
      "Train Epoch: 15 [22784/25000 (91%)]\tLoss: 0.336073\n",
      "\n",
      "Test set: Average loss: 0.3683, Accuracy: 20912/25000 (84%)\n",
      "acc is: 0.8365, best acc is 0.8365\n",
      "\n",
      "Train Epoch: 16 [2304/25000 (9%)]\tLoss: 0.369515\n",
      "Train Epoch: 16 [4864/25000 (19%)]\tLoss: 0.345784\n",
      "Train Epoch: 16 [7424/25000 (30%)]\tLoss: 0.327813\n",
      "Train Epoch: 16 [9984/25000 (40%)]\tLoss: 0.304087\n",
      "Train Epoch: 16 [12544/25000 (50%)]\tLoss: 0.291563\n",
      "Train Epoch: 16 [15104/25000 (60%)]\tLoss: 0.352671\n",
      "Train Epoch: 16 [17664/25000 (70%)]\tLoss: 0.295074\n",
      "Train Epoch: 16 [20224/25000 (81%)]\tLoss: 0.338958\n",
      "Train Epoch: 16 [22784/25000 (91%)]\tLoss: 0.327773\n",
      "\n",
      "Test set: Average loss: 0.3789, Accuracy: 20812/25000 (83%)\n",
      "acc is: 0.8325, best acc is 0.8365\n",
      "\n",
      "Train Epoch: 17 [2304/25000 (9%)]\tLoss: 0.363033\n",
      "Train Epoch: 17 [4864/25000 (19%)]\tLoss: 0.364948\n",
      "Train Epoch: 17 [7424/25000 (30%)]\tLoss: 0.291070\n",
      "Train Epoch: 17 [9984/25000 (40%)]\tLoss: 0.328351\n",
      "Train Epoch: 17 [12544/25000 (50%)]\tLoss: 0.302117\n",
      "Train Epoch: 17 [15104/25000 (60%)]\tLoss: 0.308445\n",
      "Train Epoch: 17 [17664/25000 (70%)]\tLoss: 0.339957\n",
      "Train Epoch: 17 [20224/25000 (81%)]\tLoss: 0.343113\n",
      "Train Epoch: 17 [22784/25000 (91%)]\tLoss: 0.437617\n",
      "\n",
      "Test set: Average loss: 0.3565, Accuracy: 21027/25000 (84%)\n",
      "acc is: 0.8411, best acc is 0.8411\n",
      "\n",
      "Train Epoch: 18 [2304/25000 (9%)]\tLoss: 0.299068\n",
      "Train Epoch: 18 [4864/25000 (19%)]\tLoss: 0.258302\n",
      "Train Epoch: 18 [7424/25000 (30%)]\tLoss: 0.313542\n",
      "Train Epoch: 18 [9984/25000 (40%)]\tLoss: 0.325029\n",
      "Train Epoch: 18 [12544/25000 (50%)]\tLoss: 0.317073\n",
      "Train Epoch: 18 [15104/25000 (60%)]\tLoss: 0.286503\n",
      "Train Epoch: 18 [17664/25000 (70%)]\tLoss: 0.338230\n",
      "Train Epoch: 18 [20224/25000 (81%)]\tLoss: 0.354778\n",
      "Train Epoch: 18 [22784/25000 (91%)]\tLoss: 0.287783\n",
      "\n",
      "Test set: Average loss: 0.3611, Accuracy: 21025/25000 (84%)\n",
      "acc is: 0.8410, best acc is 0.8411\n",
      "\n",
      "Train Epoch: 19 [2304/25000 (9%)]\tLoss: 0.295623\n",
      "Train Epoch: 19 [4864/25000 (19%)]\tLoss: 0.337073\n",
      "Train Epoch: 19 [7424/25000 (30%)]\tLoss: 0.345367\n",
      "Train Epoch: 19 [9984/25000 (40%)]\tLoss: 0.319353\n",
      "Train Epoch: 19 [12544/25000 (50%)]\tLoss: 0.313751\n",
      "Train Epoch: 19 [15104/25000 (60%)]\tLoss: 0.319373\n",
      "Train Epoch: 19 [17664/25000 (70%)]\tLoss: 0.353866\n",
      "Train Epoch: 19 [20224/25000 (81%)]\tLoss: 0.369897\n",
      "Train Epoch: 19 [22784/25000 (91%)]\tLoss: 0.328018\n",
      "\n",
      "Test set: Average loss: 0.3589, Accuracy: 21078/25000 (84%)\n",
      "acc is: 0.8431, best acc is 0.8431\n",
      "\n",
      "Train Epoch: 20 [2304/25000 (9%)]\tLoss: 0.336948\n",
      "Train Epoch: 20 [4864/25000 (19%)]\tLoss: 0.364330\n",
      "Train Epoch: 20 [7424/25000 (30%)]\tLoss: 0.260688\n",
      "Train Epoch: 20 [9984/25000 (40%)]\tLoss: 0.297361\n",
      "Train Epoch: 20 [12544/25000 (50%)]\tLoss: 0.304295\n",
      "Train Epoch: 20 [15104/25000 (60%)]\tLoss: 0.372029\n",
      "Train Epoch: 20 [17664/25000 (70%)]\tLoss: 0.301872\n",
      "Train Epoch: 20 [20224/25000 (81%)]\tLoss: 0.377500\n",
      "Train Epoch: 20 [22784/25000 (91%)]\tLoss: 0.303188\n",
      "\n",
      "Test set: Average loss: 0.3737, Accuracy: 20951/25000 (84%)\n",
      "acc is: 0.8380, best acc is 0.8431\n",
      "\n",
      "Train Epoch: 21 [2304/25000 (9%)]\tLoss: 0.325650\n",
      "Train Epoch: 21 [4864/25000 (19%)]\tLoss: 0.304507\n",
      "Train Epoch: 21 [7424/25000 (30%)]\tLoss: 0.330156\n",
      "Train Epoch: 21 [9984/25000 (40%)]\tLoss: 0.258497\n",
      "Train Epoch: 21 [12544/25000 (50%)]\tLoss: 0.297421\n",
      "Train Epoch: 21 [15104/25000 (60%)]\tLoss: 0.310411\n",
      "Train Epoch: 21 [17664/25000 (70%)]\tLoss: 0.306578\n",
      "Train Epoch: 21 [20224/25000 (81%)]\tLoss: 0.354269\n",
      "Train Epoch: 21 [22784/25000 (91%)]\tLoss: 0.308723\n",
      "\n",
      "Test set: Average loss: 0.3533, Accuracy: 21175/25000 (85%)\n",
      "acc is: 0.8470, best acc is 0.8470\n",
      "\n",
      "Train Epoch: 22 [2304/25000 (9%)]\tLoss: 0.316498\n",
      "Train Epoch: 22 [4864/25000 (19%)]\tLoss: 0.298316\n",
      "Train Epoch: 22 [7424/25000 (30%)]\tLoss: 0.313624\n",
      "Train Epoch: 22 [9984/25000 (40%)]\tLoss: 0.355776\n",
      "Train Epoch: 22 [12544/25000 (50%)]\tLoss: 0.312700\n",
      "Train Epoch: 22 [15104/25000 (60%)]\tLoss: 0.235453\n",
      "Train Epoch: 22 [17664/25000 (70%)]\tLoss: 0.337061\n",
      "Train Epoch: 22 [20224/25000 (81%)]\tLoss: 0.286284\n",
      "Train Epoch: 22 [22784/25000 (91%)]\tLoss: 0.256732\n",
      "\n",
      "Test set: Average loss: 0.3521, Accuracy: 21166/25000 (85%)\n",
      "acc is: 0.8466, best acc is 0.8470\n",
      "\n",
      "Train Epoch: 23 [2304/25000 (9%)]\tLoss: 0.312985\n",
      "Train Epoch: 23 [4864/25000 (19%)]\tLoss: 0.272125\n",
      "Train Epoch: 23 [7424/25000 (30%)]\tLoss: 0.351560\n",
      "Train Epoch: 23 [9984/25000 (40%)]\tLoss: 0.314145\n",
      "Train Epoch: 23 [12544/25000 (50%)]\tLoss: 0.316036\n",
      "Train Epoch: 23 [15104/25000 (60%)]\tLoss: 0.255786\n",
      "Train Epoch: 23 [17664/25000 (70%)]\tLoss: 0.350376\n",
      "Train Epoch: 23 [20224/25000 (81%)]\tLoss: 0.323474\n",
      "Train Epoch: 23 [22784/25000 (91%)]\tLoss: 0.338802\n",
      "\n",
      "Test set: Average loss: 0.3513, Accuracy: 21184/25000 (85%)\n",
      "acc is: 0.8474, best acc is 0.8474\n",
      "\n",
      "Train Epoch: 24 [2304/25000 (9%)]\tLoss: 0.286082\n",
      "Train Epoch: 24 [4864/25000 (19%)]\tLoss: 0.332519\n",
      "Train Epoch: 24 [7424/25000 (30%)]\tLoss: 0.345741\n",
      "Train Epoch: 24 [9984/25000 (40%)]\tLoss: 0.288245\n",
      "Train Epoch: 24 [12544/25000 (50%)]\tLoss: 0.325085\n",
      "Train Epoch: 24 [15104/25000 (60%)]\tLoss: 0.228994\n",
      "Train Epoch: 24 [17664/25000 (70%)]\tLoss: 0.326800\n",
      "Train Epoch: 24 [20224/25000 (81%)]\tLoss: 0.278055\n",
      "Train Epoch: 24 [22784/25000 (91%)]\tLoss: 0.274467\n",
      "\n",
      "Test set: Average loss: 0.3527, Accuracy: 21178/25000 (85%)\n",
      "acc is: 0.8471, best acc is 0.8474\n",
      "\n",
      "Train Epoch: 25 [2304/25000 (9%)]\tLoss: 0.264045\n",
      "Train Epoch: 25 [4864/25000 (19%)]\tLoss: 0.291651\n",
      "Train Epoch: 25 [7424/25000 (30%)]\tLoss: 0.310646\n",
      "Train Epoch: 25 [9984/25000 (40%)]\tLoss: 0.248906\n",
      "Train Epoch: 25 [12544/25000 (50%)]\tLoss: 0.383627\n",
      "Train Epoch: 25 [15104/25000 (60%)]\tLoss: 0.262367\n",
      "Train Epoch: 25 [17664/25000 (70%)]\tLoss: 0.303678\n",
      "Train Epoch: 25 [20224/25000 (81%)]\tLoss: 0.301971\n",
      "Train Epoch: 25 [22784/25000 (91%)]\tLoss: 0.292570\n",
      "\n",
      "Test set: Average loss: 0.3505, Accuracy: 21206/25000 (85%)\n",
      "acc is: 0.8482, best acc is 0.8482\n",
      "\n",
      "Train Epoch: 26 [2304/25000 (9%)]\tLoss: 0.246558\n",
      "Train Epoch: 26 [4864/25000 (19%)]\tLoss: 0.334325\n",
      "Train Epoch: 26 [7424/25000 (30%)]\tLoss: 0.288364\n",
      "Train Epoch: 26 [9984/25000 (40%)]\tLoss: 0.252914\n",
      "Train Epoch: 26 [12544/25000 (50%)]\tLoss: 0.270044\n",
      "Train Epoch: 26 [15104/25000 (60%)]\tLoss: 0.335304\n",
      "Train Epoch: 26 [17664/25000 (70%)]\tLoss: 0.317139\n",
      "Train Epoch: 26 [20224/25000 (81%)]\tLoss: 0.351045\n",
      "Train Epoch: 26 [22784/25000 (91%)]\tLoss: 0.286707\n",
      "\n",
      "Test set: Average loss: 0.3505, Accuracy: 21207/25000 (85%)\n",
      "acc is: 0.8483, best acc is 0.8483\n",
      "\n",
      "Train Epoch: 27 [2304/25000 (9%)]\tLoss: 0.343017\n",
      "Train Epoch: 27 [4864/25000 (19%)]\tLoss: 0.289195\n",
      "Train Epoch: 27 [7424/25000 (30%)]\tLoss: 0.234745\n",
      "Train Epoch: 27 [9984/25000 (40%)]\tLoss: 0.312097\n",
      "Train Epoch: 27 [12544/25000 (50%)]\tLoss: 0.318089\n",
      "Train Epoch: 27 [15104/25000 (60%)]\tLoss: 0.304538\n",
      "Train Epoch: 27 [17664/25000 (70%)]\tLoss: 0.311260\n",
      "Train Epoch: 27 [20224/25000 (81%)]\tLoss: 0.223191\n",
      "Train Epoch: 27 [22784/25000 (91%)]\tLoss: 0.275717\n",
      "\n",
      "Test set: Average loss: 0.3516, Accuracy: 21197/25000 (85%)\n",
      "acc is: 0.8479, best acc is 0.8483\n",
      "\n",
      "Train Epoch: 28 [2304/25000 (9%)]\tLoss: 0.333943\n",
      "Train Epoch: 28 [4864/25000 (19%)]\tLoss: 0.282528\n",
      "Train Epoch: 28 [7424/25000 (30%)]\tLoss: 0.293197\n",
      "Train Epoch: 28 [9984/25000 (40%)]\tLoss: 0.290470\n",
      "Train Epoch: 28 [12544/25000 (50%)]\tLoss: 0.294245\n",
      "Train Epoch: 28 [15104/25000 (60%)]\tLoss: 0.264078\n",
      "Train Epoch: 28 [17664/25000 (70%)]\tLoss: 0.328334\n",
      "Train Epoch: 28 [20224/25000 (81%)]\tLoss: 0.323549\n",
      "Train Epoch: 28 [22784/25000 (91%)]\tLoss: 0.357181\n",
      "\n",
      "Test set: Average loss: 0.3535, Accuracy: 21188/25000 (85%)\n",
      "acc is: 0.8475, best acc is 0.8483\n",
      "\n",
      "Train Epoch: 29 [2304/25000 (9%)]\tLoss: 0.339110\n",
      "Train Epoch: 29 [4864/25000 (19%)]\tLoss: 0.247445\n",
      "Train Epoch: 29 [7424/25000 (30%)]\tLoss: 0.250841\n",
      "Train Epoch: 29 [9984/25000 (40%)]\tLoss: 0.317017\n",
      "Train Epoch: 29 [12544/25000 (50%)]\tLoss: 0.313871\n",
      "Train Epoch: 29 [15104/25000 (60%)]\tLoss: 0.206379\n",
      "Train Epoch: 29 [17664/25000 (70%)]\tLoss: 0.298737\n",
      "Train Epoch: 29 [20224/25000 (81%)]\tLoss: 0.332279\n",
      "Train Epoch: 29 [22784/25000 (91%)]\tLoss: 0.295760\n",
      "\n",
      "Test set: Average loss: 0.3485, Accuracy: 21218/25000 (85%)\n",
      "acc is: 0.8487, best acc is 0.8487\n",
      "\n",
      "Train Epoch: 30 [2304/25000 (9%)]\tLoss: 0.260942\n",
      "Train Epoch: 30 [4864/25000 (19%)]\tLoss: 0.296700\n",
      "Train Epoch: 30 [7424/25000 (30%)]\tLoss: 0.346731\n",
      "Train Epoch: 30 [9984/25000 (40%)]\tLoss: 0.359506\n",
      "Train Epoch: 30 [12544/25000 (50%)]\tLoss: 0.257005\n",
      "Train Epoch: 30 [15104/25000 (60%)]\tLoss: 0.288363\n",
      "Train Epoch: 30 [17664/25000 (70%)]\tLoss: 0.303621\n",
      "Train Epoch: 30 [20224/25000 (81%)]\tLoss: 0.299061\n",
      "Train Epoch: 30 [22784/25000 (91%)]\tLoss: 0.322581\n",
      "\n",
      "Test set: Average loss: 0.3518, Accuracy: 21211/25000 (85%)\n",
      "acc is: 0.8484, best acc is 0.8487\n",
      "\n",
      "Train Epoch: 31 [2304/25000 (9%)]\tLoss: 0.321970\n",
      "Train Epoch: 31 [4864/25000 (19%)]\tLoss: 0.272169\n",
      "Train Epoch: 31 [7424/25000 (30%)]\tLoss: 0.308363\n",
      "Train Epoch: 31 [9984/25000 (40%)]\tLoss: 0.322722\n",
      "Train Epoch: 31 [12544/25000 (50%)]\tLoss: 0.268164\n",
      "Train Epoch: 31 [15104/25000 (60%)]\tLoss: 0.265954\n",
      "Train Epoch: 31 [17664/25000 (70%)]\tLoss: 0.365964\n",
      "Train Epoch: 31 [20224/25000 (81%)]\tLoss: 0.293997\n",
      "Train Epoch: 31 [22784/25000 (91%)]\tLoss: 0.279895\n",
      "\n",
      "Test set: Average loss: 0.3460, Accuracy: 21246/25000 (85%)\n",
      "acc is: 0.8498, best acc is 0.8498\n",
      "\n",
      "Train Epoch: 32 [2304/25000 (9%)]\tLoss: 0.263390\n",
      "Train Epoch: 32 [4864/25000 (19%)]\tLoss: 0.285811\n",
      "Train Epoch: 32 [7424/25000 (30%)]\tLoss: 0.274666\n",
      "Train Epoch: 32 [9984/25000 (40%)]\tLoss: 0.388116\n",
      "Train Epoch: 32 [12544/25000 (50%)]\tLoss: 0.323773\n",
      "Train Epoch: 32 [15104/25000 (60%)]\tLoss: 0.301281\n",
      "Train Epoch: 32 [17664/25000 (70%)]\tLoss: 0.328159\n",
      "Train Epoch: 32 [20224/25000 (81%)]\tLoss: 0.297623\n",
      "Train Epoch: 32 [22784/25000 (91%)]\tLoss: 0.353789\n",
      "\n",
      "Test set: Average loss: 0.3543, Accuracy: 21196/25000 (85%)\n",
      "acc is: 0.8478, best acc is 0.8498\n",
      "\n",
      "Train Epoch: 33 [2304/25000 (9%)]\tLoss: 0.230411\n",
      "Train Epoch: 33 [4864/25000 (19%)]\tLoss: 0.369552\n",
      "Train Epoch: 33 [7424/25000 (30%)]\tLoss: 0.361458\n",
      "Train Epoch: 33 [9984/25000 (40%)]\tLoss: 0.272258\n",
      "Train Epoch: 33 [12544/25000 (50%)]\tLoss: 0.378121\n",
      "Train Epoch: 33 [15104/25000 (60%)]\tLoss: 0.349469\n",
      "Train Epoch: 33 [17664/25000 (70%)]\tLoss: 0.298434\n",
      "Train Epoch: 33 [20224/25000 (81%)]\tLoss: 0.300441\n",
      "Train Epoch: 33 [22784/25000 (91%)]\tLoss: 0.305963\n",
      "\n",
      "Test set: Average loss: 0.3604, Accuracy: 21163/25000 (85%)\n",
      "acc is: 0.8465, best acc is 0.8498\n",
      "\n",
      "Train Epoch: 34 [2304/25000 (9%)]\tLoss: 0.236183\n",
      "Train Epoch: 34 [4864/25000 (19%)]\tLoss: 0.362474\n",
      "Train Epoch: 34 [7424/25000 (30%)]\tLoss: 0.306393\n",
      "Train Epoch: 34 [9984/25000 (40%)]\tLoss: 0.254436\n",
      "Train Epoch: 34 [12544/25000 (50%)]\tLoss: 0.278256\n",
      "Train Epoch: 34 [15104/25000 (60%)]\tLoss: 0.265687\n",
      "Train Epoch: 34 [17664/25000 (70%)]\tLoss: 0.279261\n",
      "Train Epoch: 34 [20224/25000 (81%)]\tLoss: 0.331426\n",
      "Train Epoch: 34 [22784/25000 (91%)]\tLoss: 0.287909\n",
      "\n",
      "Test set: Average loss: 0.3497, Accuracy: 21271/25000 (85%)\n",
      "acc is: 0.8508, best acc is 0.8508\n",
      "\n",
      "Train Epoch: 35 [2304/25000 (9%)]\tLoss: 0.301747\n",
      "Train Epoch: 35 [4864/25000 (19%)]\tLoss: 0.297050\n",
      "Train Epoch: 35 [7424/25000 (30%)]\tLoss: 0.240084\n",
      "Train Epoch: 35 [9984/25000 (40%)]\tLoss: 0.261711\n",
      "Train Epoch: 35 [12544/25000 (50%)]\tLoss: 0.314194\n",
      "Train Epoch: 35 [15104/25000 (60%)]\tLoss: 0.277936\n",
      "Train Epoch: 35 [17664/25000 (70%)]\tLoss: 0.265545\n",
      "Train Epoch: 35 [20224/25000 (81%)]\tLoss: 0.276528\n",
      "Train Epoch: 35 [22784/25000 (91%)]\tLoss: 0.301039\n",
      "\n",
      "Test set: Average loss: 0.3473, Accuracy: 21297/25000 (85%)\n",
      "acc is: 0.8519, best acc is 0.8519\n",
      "\n",
      "Train Epoch: 36 [2304/25000 (9%)]\tLoss: 0.317722\n",
      "Train Epoch: 36 [4864/25000 (19%)]\tLoss: 0.307372\n",
      "Train Epoch: 36 [7424/25000 (30%)]\tLoss: 0.291381\n",
      "Train Epoch: 36 [9984/25000 (40%)]\tLoss: 0.292480\n",
      "Train Epoch: 36 [12544/25000 (50%)]\tLoss: 0.313562\n",
      "Train Epoch: 36 [15104/25000 (60%)]\tLoss: 0.333353\n",
      "Train Epoch: 36 [17664/25000 (70%)]\tLoss: 0.258586\n",
      "Train Epoch: 36 [20224/25000 (81%)]\tLoss: 0.286908\n",
      "Train Epoch: 36 [22784/25000 (91%)]\tLoss: 0.336932\n",
      "\n",
      "Test set: Average loss: 0.3374, Accuracy: 21354/25000 (85%)\n",
      "acc is: 0.8542, best acc is 0.8542\n",
      "\n",
      "Train Epoch: 37 [2304/25000 (9%)]\tLoss: 0.243541\n",
      "Train Epoch: 37 [4864/25000 (19%)]\tLoss: 0.269255\n",
      "Train Epoch: 37 [7424/25000 (30%)]\tLoss: 0.296576\n",
      "Train Epoch: 37 [9984/25000 (40%)]\tLoss: 0.365936\n",
      "Train Epoch: 37 [12544/25000 (50%)]\tLoss: 0.329725\n",
      "Train Epoch: 37 [15104/25000 (60%)]\tLoss: 0.278975\n",
      "Train Epoch: 37 [17664/25000 (70%)]\tLoss: 0.276773\n",
      "Train Epoch: 37 [20224/25000 (81%)]\tLoss: 0.263693\n",
      "Train Epoch: 37 [22784/25000 (91%)]\tLoss: 0.243503\n",
      "\n",
      "Test set: Average loss: 0.3388, Accuracy: 21395/25000 (86%)\n",
      "acc is: 0.8558, best acc is 0.8558\n",
      "\n",
      "Train Epoch: 38 [2304/25000 (9%)]\tLoss: 0.286390\n",
      "Train Epoch: 38 [4864/25000 (19%)]\tLoss: 0.291440\n",
      "Train Epoch: 38 [7424/25000 (30%)]\tLoss: 0.334766\n",
      "Train Epoch: 38 [9984/25000 (40%)]\tLoss: 0.303078\n",
      "Train Epoch: 38 [12544/25000 (50%)]\tLoss: 0.233663\n",
      "Train Epoch: 38 [15104/25000 (60%)]\tLoss: 0.313233\n",
      "Train Epoch: 38 [17664/25000 (70%)]\tLoss: 0.313456\n",
      "Train Epoch: 38 [20224/25000 (81%)]\tLoss: 0.331353\n",
      "Train Epoch: 38 [22784/25000 (91%)]\tLoss: 0.295682\n",
      "\n",
      "Test set: Average loss: 0.3373, Accuracy: 21431/25000 (86%)\n",
      "acc is: 0.8572, best acc is 0.8572\n",
      "\n",
      "Train Epoch: 39 [2304/25000 (9%)]\tLoss: 0.296772\n",
      "Train Epoch: 39 [4864/25000 (19%)]\tLoss: 0.300696\n",
      "Train Epoch: 39 [7424/25000 (30%)]\tLoss: 0.285488\n",
      "Train Epoch: 39 [9984/25000 (40%)]\tLoss: 0.242687\n",
      "Train Epoch: 39 [12544/25000 (50%)]\tLoss: 0.284849\n",
      "Train Epoch: 39 [15104/25000 (60%)]\tLoss: 0.275956\n",
      "Train Epoch: 39 [17664/25000 (70%)]\tLoss: 0.340791\n",
      "Train Epoch: 39 [20224/25000 (81%)]\tLoss: 0.333146\n",
      "Train Epoch: 39 [22784/25000 (91%)]\tLoss: 0.294060\n",
      "\n",
      "Test set: Average loss: 0.3447, Accuracy: 21408/25000 (86%)\n",
      "acc is: 0.8563, best acc is 0.8572\n",
      "\n",
      "Train Epoch: 40 [2304/25000 (9%)]\tLoss: 0.271093\n",
      "Train Epoch: 40 [4864/25000 (19%)]\tLoss: 0.229742\n",
      "Train Epoch: 40 [7424/25000 (30%)]\tLoss: 0.357732\n",
      "Train Epoch: 40 [9984/25000 (40%)]\tLoss: 0.299999\n",
      "Train Epoch: 40 [12544/25000 (50%)]\tLoss: 0.266848\n",
      "Train Epoch: 40 [15104/25000 (60%)]\tLoss: 0.277095\n",
      "Train Epoch: 40 [17664/25000 (70%)]\tLoss: 0.296720\n",
      "Train Epoch: 40 [20224/25000 (81%)]\tLoss: 0.261047\n",
      "Train Epoch: 40 [22784/25000 (91%)]\tLoss: 0.264592\n",
      "\n",
      "Test set: Average loss: 0.3327, Accuracy: 21447/25000 (86%)\n",
      "acc is: 0.8579, best acc is 0.8579\n",
      "\n",
      "Train Epoch: 41 [2304/25000 (9%)]\tLoss: 0.275941\n",
      "Train Epoch: 41 [4864/25000 (19%)]\tLoss: 0.271695\n",
      "Train Epoch: 41 [7424/25000 (30%)]\tLoss: 0.248363\n",
      "Train Epoch: 41 [9984/25000 (40%)]\tLoss: 0.270569\n",
      "Train Epoch: 41 [12544/25000 (50%)]\tLoss: 0.333697\n",
      "Train Epoch: 41 [15104/25000 (60%)]\tLoss: 0.284244\n",
      "Train Epoch: 41 [17664/25000 (70%)]\tLoss: 0.320641\n",
      "Train Epoch: 41 [20224/25000 (81%)]\tLoss: 0.292103\n",
      "Train Epoch: 41 [22784/25000 (91%)]\tLoss: 0.263534\n",
      "\n",
      "Test set: Average loss: 0.3515, Accuracy: 21356/25000 (85%)\n",
      "acc is: 0.8542, best acc is 0.8579\n",
      "\n",
      "Train Epoch: 42 [2304/25000 (9%)]\tLoss: 0.224505\n",
      "Train Epoch: 42 [4864/25000 (19%)]\tLoss: 0.297998\n",
      "Train Epoch: 42 [7424/25000 (30%)]\tLoss: 0.381009\n",
      "Train Epoch: 42 [9984/25000 (40%)]\tLoss: 0.296687\n",
      "Train Epoch: 42 [12544/25000 (50%)]\tLoss: 0.293068\n",
      "Train Epoch: 42 [15104/25000 (60%)]\tLoss: 0.257831\n",
      "Train Epoch: 42 [17664/25000 (70%)]\tLoss: 0.303423\n",
      "Train Epoch: 42 [20224/25000 (81%)]\tLoss: 0.299366\n",
      "Train Epoch: 42 [22784/25000 (91%)]\tLoss: 0.293169\n",
      "\n",
      "Test set: Average loss: 0.3453, Accuracy: 21366/25000 (85%)\n",
      "acc is: 0.8546, best acc is 0.8579\n",
      "\n",
      "Train Epoch: 43 [2304/25000 (9%)]\tLoss: 0.309538\n",
      "Train Epoch: 43 [4864/25000 (19%)]\tLoss: 0.198327\n",
      "Train Epoch: 43 [7424/25000 (30%)]\tLoss: 0.250175\n",
      "Train Epoch: 43 [9984/25000 (40%)]\tLoss: 0.336082\n",
      "Train Epoch: 43 [12544/25000 (50%)]\tLoss: 0.249495\n",
      "Train Epoch: 43 [15104/25000 (60%)]\tLoss: 0.282267\n",
      "Train Epoch: 43 [17664/25000 (70%)]\tLoss: 0.268738\n",
      "Train Epoch: 43 [20224/25000 (81%)]\tLoss: 0.266380\n",
      "Train Epoch: 43 [22784/25000 (91%)]\tLoss: 0.274753\n",
      "\n",
      "Test set: Average loss: 0.3577, Accuracy: 21229/25000 (85%)\n",
      "acc is: 0.8492, best acc is 0.8579\n",
      "\n",
      "Train Epoch: 44 [2304/25000 (9%)]\tLoss: 0.287805\n",
      "Train Epoch: 44 [4864/25000 (19%)]\tLoss: 0.234868\n",
      "Train Epoch: 44 [7424/25000 (30%)]\tLoss: 0.227574\n",
      "Train Epoch: 44 [9984/25000 (40%)]\tLoss: 0.216840\n",
      "Train Epoch: 44 [12544/25000 (50%)]\tLoss: 0.231119\n",
      "Train Epoch: 44 [15104/25000 (60%)]\tLoss: 0.297107\n",
      "Train Epoch: 44 [17664/25000 (70%)]\tLoss: 0.284339\n",
      "Train Epoch: 44 [20224/25000 (81%)]\tLoss: 0.272087\n",
      "Train Epoch: 44 [22784/25000 (91%)]\tLoss: 0.205972\n",
      "\n",
      "Test set: Average loss: 0.3435, Accuracy: 21452/25000 (86%)\n",
      "acc is: 0.8581, best acc is 0.8581\n",
      "\n",
      "Train Epoch: 45 [2304/25000 (9%)]\tLoss: 0.218207\n",
      "Train Epoch: 45 [4864/25000 (19%)]\tLoss: 0.259078\n",
      "Train Epoch: 45 [7424/25000 (30%)]\tLoss: 0.277737\n",
      "Train Epoch: 45 [9984/25000 (40%)]\tLoss: 0.196511\n",
      "Train Epoch: 45 [12544/25000 (50%)]\tLoss: 0.260746\n",
      "Train Epoch: 45 [15104/25000 (60%)]\tLoss: 0.318969\n",
      "Train Epoch: 45 [17664/25000 (70%)]\tLoss: 0.259293\n",
      "Train Epoch: 45 [20224/25000 (81%)]\tLoss: 0.253208\n",
      "Train Epoch: 45 [22784/25000 (91%)]\tLoss: 0.239189\n",
      "\n",
      "Test set: Average loss: 0.3339, Accuracy: 21554/25000 (86%)\n",
      "acc is: 0.8622, best acc is 0.8622\n",
      "\n",
      "Train Epoch: 46 [2304/25000 (9%)]\tLoss: 0.243912\n",
      "Train Epoch: 46 [4864/25000 (19%)]\tLoss: 0.227510\n",
      "Train Epoch: 46 [7424/25000 (30%)]\tLoss: 0.267616\n",
      "Train Epoch: 46 [9984/25000 (40%)]\tLoss: 0.227165\n",
      "Train Epoch: 46 [12544/25000 (50%)]\tLoss: 0.183347\n",
      "Train Epoch: 46 [15104/25000 (60%)]\tLoss: 0.236498\n",
      "Train Epoch: 46 [17664/25000 (70%)]\tLoss: 0.296210\n",
      "Train Epoch: 46 [20224/25000 (81%)]\tLoss: 0.220213\n",
      "Train Epoch: 46 [22784/25000 (91%)]\tLoss: 0.235895\n",
      "\n",
      "Test set: Average loss: 0.3405, Accuracy: 21521/25000 (86%)\n",
      "acc is: 0.8608, best acc is 0.8622\n",
      "\n",
      "Train Epoch: 47 [2304/25000 (9%)]\tLoss: 0.236041\n",
      "Train Epoch: 47 [4864/25000 (19%)]\tLoss: 0.200200\n",
      "Train Epoch: 47 [7424/25000 (30%)]\tLoss: 0.215517\n",
      "Train Epoch: 47 [9984/25000 (40%)]\tLoss: 0.218557\n",
      "Train Epoch: 47 [12544/25000 (50%)]\tLoss: 0.204843\n",
      "Train Epoch: 47 [15104/25000 (60%)]\tLoss: 0.226706\n",
      "Train Epoch: 47 [17664/25000 (70%)]\tLoss: 0.212414\n",
      "Train Epoch: 47 [20224/25000 (81%)]\tLoss: 0.263971\n",
      "Train Epoch: 47 [22784/25000 (91%)]\tLoss: 0.266034\n",
      "\n",
      "Test set: Average loss: 0.3748, Accuracy: 21288/25000 (85%)\n",
      "acc is: 0.8515, best acc is 0.8622\n",
      "\n",
      "Train Epoch: 48 [2304/25000 (9%)]\tLoss: 0.151262\n",
      "Train Epoch: 48 [4864/25000 (19%)]\tLoss: 0.259424\n",
      "Train Epoch: 48 [7424/25000 (30%)]\tLoss: 0.227220\n",
      "Train Epoch: 48 [9984/25000 (40%)]\tLoss: 0.230428\n",
      "Train Epoch: 48 [12544/25000 (50%)]\tLoss: 0.173710\n",
      "Train Epoch: 48 [15104/25000 (60%)]\tLoss: 0.227834\n",
      "Train Epoch: 48 [17664/25000 (70%)]\tLoss: 0.228147\n",
      "Train Epoch: 48 [20224/25000 (81%)]\tLoss: 0.314192\n",
      "Train Epoch: 48 [22784/25000 (91%)]\tLoss: 0.281375\n",
      "\n",
      "Test set: Average loss: 0.3396, Accuracy: 21512/25000 (86%)\n",
      "acc is: 0.8605, best acc is 0.8622\n",
      "\n",
      "Train Epoch: 49 [2304/25000 (9%)]\tLoss: 0.230221\n",
      "Train Epoch: 49 [4864/25000 (19%)]\tLoss: 0.212407\n",
      "Train Epoch: 49 [7424/25000 (30%)]\tLoss: 0.209213\n",
      "Train Epoch: 49 [9984/25000 (40%)]\tLoss: 0.189342\n",
      "Train Epoch: 49 [12544/25000 (50%)]\tLoss: 0.250502\n",
      "Train Epoch: 49 [15104/25000 (60%)]\tLoss: 0.265183\n",
      "Train Epoch: 49 [17664/25000 (70%)]\tLoss: 0.242383\n",
      "Train Epoch: 49 [20224/25000 (81%)]\tLoss: 0.247975\n",
      "Train Epoch: 49 [22784/25000 (91%)]\tLoss: 0.248671\n",
      "\n",
      "Test set: Average loss: 0.3578, Accuracy: 21387/25000 (86%)\n",
      "acc is: 0.8555, best acc is 0.8622\n",
      "\n",
      "Train Epoch: 50 [2304/25000 (9%)]\tLoss: 0.141374\n",
      "Train Epoch: 50 [4864/25000 (19%)]\tLoss: 0.234654\n",
      "Train Epoch: 50 [7424/25000 (30%)]\tLoss: 0.244952\n",
      "Train Epoch: 50 [9984/25000 (40%)]\tLoss: 0.222427\n",
      "Train Epoch: 50 [12544/25000 (50%)]\tLoss: 0.189970\n",
      "Train Epoch: 50 [15104/25000 (60%)]\tLoss: 0.193003\n",
      "Train Epoch: 50 [17664/25000 (70%)]\tLoss: 0.226010\n",
      "Train Epoch: 50 [20224/25000 (81%)]\tLoss: 0.185273\n",
      "Train Epoch: 50 [22784/25000 (91%)]\tLoss: 0.270341\n",
      "\n",
      "Test set: Average loss: 0.3739, Accuracy: 21053/25000 (84%)\n",
      "acc is: 0.8421, best acc is 0.8622\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:1') #\n",
    "EMB_SIZE = 128   # embedding size\n",
    "HIDDEN_SIZE = 128   # embedding size\n",
    "INITIAL_LEARNING_RATE = 1e-3\n",
    "EPOCH = 50\n",
    "trasformer = MultiLayerTransformer(input_size=MAX_WORDS, embed_size=EMB_SIZE,hidden_size=HIDDEN_SIZE, \n",
    "                              num_layers=2, output_size=2).to(DEVICE)\n",
    "\n",
    "print(trasformer)\n",
    "optimizer = optim.Adam(trasformer.parameters(), lr=INITIAL_LEARNING_RATE, \n",
    "                       betas=(0.9, 0.995), eps=1e-08, weight_decay=1e-4)\n",
    "best_acc = 0.0 \n",
    "PATH = 'model/transformer_model.pth'  # 定义模型保存路径\n",
    "\n",
    "acc_list = []\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCH//2, eta_min=1e-8)\n",
    "for epoch in range(1, EPOCH+1):  # 10个epoch\n",
    "    train(trasformer, DEVICE, train_loader, optimizer, epoch)\n",
    "    acc = test(trasformer, DEVICE, test_loader)\n",
    "    acc_list.append(acc)\n",
    "    if best_acc < acc: \n",
    "        best_acc = acc \n",
    "        torch.save(trasformer.state_dict(), PATH)\n",
    "    print(\"acc is: {:.4f}, best acc is {:.4f}\\n\".format(acc, best_acc)) \n",
    "    scheduler.step()\n",
    "    \n",
    "plt.plot(acc_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3339, Accuracy: 21554/25000 (86%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86216"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检验保存的模型\n",
    "best_model = MultiLayerTransformer(input_size=MAX_WORDS, embed_size=EMB_SIZE,hidden_size=HIDDEN_SIZE, \n",
    "                              num_layers=2, output_size=2).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(PATH))\n",
    "test(best_model, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业要求\n",
    "\n",
    "1. 按照给出的公式，完成attention函数以及single_head_attention函数\n",
    "2. 对lstm + attention的超参数(Batch Size, epoch数，学习率，词向量编码长度等)进行联合调优，展示优化过程。\n",
    "3. 实现一个简单的Transformer，可以进一步从以下几方面或网络结构，超参数等对结果进行改进。\n",
    "    - 对位置编码进行改进\n",
    "    - 将自注意力机制改为多头注意力机制\n",
    "    - 对Eecoder/decoder部分进行优化\n",
    "### 提交形式：pdf格式的报告，并在报告中附上源代码。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
